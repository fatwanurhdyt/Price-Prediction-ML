# -*- coding: utf-8 -*-
"""price-prediction-ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ei6wsFhYMDLs5-x3Y69TR0SPAdMOs59W

# **Predictive Analytics: USA House Price Prediction**

- **Author:** Fatwa Nurhidayat
- **Email:** fatwa.nrhdyt@gmail.com

# **Project Domain**

## **Background**
The real estate market serves as a critical cornerstone of a nation's economic stability and growth. Property valuations are determined by a complex interplay of multi-dimensional factors, ranging from physical attributes—such as the number of bedrooms, bathrooms, and total living area—to external variables like geographical location and structural condition.

Temporal elements, including the year of construction and recent renovation history, also play a significant role in determining a property's market value. Furthermore, premium features such as basement availability, aesthetic views, and waterfront proximity act as high-impact value drivers in the valuation process.

A study by **Yahya et al. (2021)** demonstrates that machine learning approaches in house price prediction are instrumental in mitigating market uncertainty and facilitating data-driven decision-making. Although the study was initially conducted in Penang, Malaysia, the methodology remains highly applicable to global urban property markets, which often share similarly complex pricing dynamics.

## **Problem Urgency**
- **Enhanced Pricing Accuracy:** Assisting potential buyers in strategic financial planning and ensuring fair market value assessments.
- **Strategic Investment Insights:** Empowering investors to identify high-yield locations and optimize property portfolios through data-driven analysis.
- **Informed Urban Planning:** Providing local authorities with predictive insights to optimize public service distribution and regional infrastructure development.

## **References**
Yahya, N., Zainuddin, N. M. M., Sjarif, N. N. A., & Azmi, N. F. M. (2021). Predictive Visual Analytics for Machine Learning Model in House Price Prediction: A Case Study. *Open International Journal of Informatics (OIJI)*, 9(1), 1–10. [http://eprints.utm.my/97493/1/NorhayatiYahya2021_PredictiveVisualAnalyticsforMachine.pdf](http://eprints.utm.my/97493/1/NorhayatiYahya2021_PredictiveVisualAnalyticsforMachine.pdf)

# **Business Understanding**

**Problem Statements**
1. **Valuation Complexity:** Real estate prices exhibit high variance driven by diverse factors such as structural attributes, physical condition, and geographical location. Potential buyers and investors often struggle to objectively assess whether a property's asking price aligns with its actual physical characteristics.
2. **Model Optimization:** Identifying a regression model capable of capturing the intricate and non-linear relationships between property features and market value remains a challenge, leading to suboptimal predictive insights.

**Goals**
1. **Decision Support:** Empowering prospective buyers and investors with a data-driven estimation tool to evaluate property values based on physical and locational features, thereby facilitating more informed and precise investment decisions.
2. **Predictive Excellence:** Systematically identifying and evaluating multiple regression algorithms to establish the most optimal model for high-precision house price forecasting based on available datasets.

**Solution Statements**<br>
To achieve the aforementioned goals, this project implements the following strategic technical phases:
- **Exploratory Data Analysis (EDA) & Data Mining:** Conducting deep visual exploration to uncover underlying patterns, correlations, and multi-dimensional relationships between property attributes and sales prices.
- **Advanced Modeling:** Building and benchmarking predictive models using **K-Nearest Neighbors (KNN)** and **XGBoost Regression** to handle both linear and complex non-linear data structures.
- **Rigorous Evaluation:** Assessing model performance utilizing the **R-squared (R²)** metric to ensure predictive reliability, precision, and alignment with business requirements.

# **Data Understanding**

## **Dataset Description**

The dataset utilized in this project provides comprehensive information regarding residential property sales in the United States. It encompasses various structural, temporal, and locational attributes that serve as critical predictors for real estate valuation.

---

### **Dataset Overview**
- **Total Features:** 18
- **Total Records:** 4,140
- **Data Source:** [Kaggle - USA House Prices](https://www.kaggle.com/datasets/fratzcan/usa-house-prices)

---

### **Data Dictionary**

| Feature | Description |
|---|---|
| `Date` | The transaction date, used for time-series trend analysis of property prices. |
| `Price` | The sale price in USD (Target Variable for prediction). |
| `Bedrooms` | Number of bedrooms; typically, higher counts correlate with higher property values. |
| `Bathrooms` | Number of bathrooms; a key factor influencing structural valuation. |
| `Sqft Living` | Total interior living space in square feet. |
| `Sqft Lot` | Total land area in square feet. |
| `Floors` | Number of floors/levels in the property. |
| `Waterfront` | Binary indicator (1 = Waterfront view, 0 = No waterfront view). |
| `View` | Quality index of the property's view (scale: 0 to 4). |
| `Condition` | Structural condition index (scale: 1 to 5). |
| `Sqft Above` | Interior housing area excluding the basement. |
| `Sqft Basement` | Total basement area; contributes to value based on functionality. |
| `Yr Built` | Year of original construction. |
| `Yr Renovated` | Year of the most recent renovation (impacts modern market appeal). |
| `Street` | Specific street address for granular location analysis. |
| `City` | The city where the property is located; accounts for localized market dynamics. |
| `Statezip` | Regional context provided through state and ZIP code combinations. |
| `Country` | Country of location (Focused on the United States for this dataset). |

---

### **Data Profiling & Exploratory Workflow**
To establish a solid foundation for predictive modeling, the following analytical steps were performed:
- **Data Ingestion:** Loading and initial inspection of the raw dataset.
- **Data Quality Assessment:** Identifying anomalies, including missing values, duplicate records, and statistical outliers.
- **Univariate Analysis:** Utilizing histograms to visualize the statistical distribution of the target variable (`Price`) and key numerical features (`Bedrooms`, `Bathrooms`, `Sqft Living`).
- **Bivariate & Multivariate Analysis:** Constructing a correlation heatmap to identify multi-collinearity and the strength of relationships between independent variables and the target price.

## **Data Ingestion**
"""

# library importation
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# local dataset loading
hprices = pd.read_csv('/Users/wafanur/submission_dicoding/submission_mlt_1/USA Housing Dataset.csv')

# data preview
hprices

# dataset information
hprices.info()

# descriptive statistics
display(hprices.describe())

"""**Insights:**
- **Price Analysis:** Property prices range from **$0 to $26.59 million**, with a mean of approximately **$553k**. The minimum value of **$0** is statistically anomalous and likely represents data entry errors or missing records. The median price (Q2) of **$460k** confirms a **right-skewed distribution**, where high-value outliers significantly pull the mean upward.
- **Bedroom Count:** Values range from 0 to 8, with an average of 3.4. Entries with **0 bedrooms** are flagged as potential data anomalies. Most properties in the dataset feature 3 to 4 bedrooms.
- **Bathroom Count:** The average is **2.16**, with a maximum of 6.75. The presence of decimal values (e.g., 1.5) reflects standard real estate notation for partial bathrooms (e.g., a full bath plus a powder room).
- **Living Area (`sqft_living`):** Featuring a median of 1,980 sqft and a maximum of 10,040 sqft, the distribution is right-skewed, indicating the presence of several luxury properties with exceptionally large living spaces.
- **Land Area (`sqft_lot`):** This feature exhibits extreme variance (std dev: 35,876), with a maximum of **1 million sqft**. This suggests a diverse portfolio ranging from standard residential lots to massive estates or agricultural lands.
- **Floor Distribution:** Properties average 1.5 floors, with a maximum of 3.5. The majority of the inventory consists of 1 to 2-story residences.
- **Waterfront Exclusivity:** Waterfront properties are rare, accounting for **less than 1%** of the total dataset (mean = 0.007), highlighting their status as premium niche assets.
- **View Quality:** Most properties lack a premium view, as evidenced by a low mean score of 0.25 on a scale of 0 to 4.
- **Structural Condition:** An average score of **3.45 (scale 1–5)** suggests that the properties are generally in good condition, with a small minority occupying the extremes of the scale.
- **Basement Analysis:** The proximity of `sqft_above` values to `sqft_living` totals indicates that most properties do not have basements, further confirmed by a **median `sqft_basement` of 0**.
- **Historical Context (`yr_built`):** Construction years range from 1900 to 2014, with an average year of **1970**. The Interquartile Range (IQR) shows that the bulk of the housing stock was built between 1951 and 1997.
- **Renovation Status:** The high frequency of zero values (median = 0) confirms that the majority of properties have **no recorded renovation history**, indicating either original condition or unlogged upgrades.

## **Data Integrity**

### **Missing Values Check**
"""

hprices.isna().sum()

"""### **Duplicate Detection**"""

hprices.duplicated().sum()

"""### **Statistical Outlier Profiling**"""

num_cols = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition',
            'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']

num_vars = hprices.shape[1]
n_cols = 4
n_rows = -(-num_vars // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))
axes = axes.flatten()

for i, column in enumerate(num_cols):
    sns.boxplot(x=hprices[column], ax=axes[i])
    axes[i].set_title(column)
    axes[i].set_xlabel('Value')

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""**Insights:**
- **Price:** Exhibits significant positive skewness due to the presence of high-magnitude extreme values.
- **Structural Space (`sqft_living`, `sqft_above`, `sqft_basement`):** These features show a distinct right-skewed distribution, indicating a concentration of standard-sized homes with a few exceptionally large properties.
- **Lot Size (`sqft_lot`):** Contains a substantial number of extreme statistical outliers, suggesting high variance in land dimensions.
- **Room Counts (`bedrooms`, `bathrooms`):** Presence of structural outliers that deviate from typical residential norms (e.g., unusually high or low room counts).

## **Numerical Feature Visualization**
"""

# configure plotting grid dimensions
n_cols = 2
n_rows = (len(num_cols) + n_cols - 1) // n_cols
plt.figure(figsize=(n_cols * 5, n_rows * 4))

# iterative histogram generation for univariate distribution analysis
for i, col in enumerate(num_cols):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.histplot(hprices[col], kde=True, color='skyblue', bins=30)
    plt.title(f'{col}')
    plt.xlabel(col)
    plt.ylabel('frequency')

plt.tight_layout()
plt.show()

"""## **Correlation Heatmap Analysis**"""

corr_matrix = hprices[num_cols].corr()
print(corr_matrix)

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Heatmap Korelasi Antar Fitur')
plt.show()

"""**Insights:**
- **Primary Price Driver:** `sqft_living` emerges as the most influential feature relative to `price` (correlation: 0.42), confirming that total interior living space is the dominant factor in property valuation.
- **Significant Predictors:** Both `sqft_above` (0.36) and `bathrooms` (0.32) demonstrate meaningful positive correlations with market value.
- **Weaker Relationships:** Interestingly, `bedrooms` shows a lower correlation of 0.19, suggesting that the number of bedrooms is a less critical predictor of price compared to total square footage.
- **Negligible Correlation:** `sqft_lot` shows nearly zero correlation with `price` (0.05), indicating that land size is not a primary value driver in this specific market segment. (*Note: Outliers for this feature will remain untreated as the column is slated for removal during the feature selection phase*).
- **Temporal & Qualitative Factors:** Features such as `condition`, `yr_built`, and `yr_renovated` exhibit very weak linear relationships with price, suggesting that age and condition are less impactful predictors within this specific dataset.
- **Multicollinearity Assessment:** Strong multicollinearity is observed between `sqft_living` and `sqft_above` (0.87), as well as `bathrooms` (0.76). This is logically consistent, as larger residences typically feature more rooms and higher-tier amenities.

# **Data Preparation**

Based on the insights gained from the Exploratory Data Analysis (EDA) phase, several critical preprocessing steps are executed to ensure data quality before model training:

- **Outlier Remediation:** Mitigating the influence of anomalous data points to prevent distortion of model parameters and evaluation metrics.
- **Feature Selection & Dimensionality Reduction:** Removing irrelevant or redundant columns to minimize computational overhead and prevent potential training errors.
- **Dataset Splitting:** Partitioning the data into Training and Testing sets (80/20) to ensure an objective evaluation and monitor for potential overfitting.
- **Data Standardization (Standard Scaling):** Normalizing the scale of numerical features to ensure distance-based and gradient-based algorithms (such as KNN and XGBoost) perform optimally.

## **Outlier Management**

Managing statistical outliers is a critical phase in the data pipeline for the following strategic reasons:

1. **Preserving Model Predictive Accuracy**
Regression models are highly sensitive to extreme values. Outliers can disproportionately influence the loss function, leading to biased coefficients and inaccurate predictions.

2. **Preventing Statistical Distortion**
Extreme values can significantly skew descriptive statistics, such as the mean and standard deviation, as well as inflate or deflate correlation coefficients. Remedying these ensures the data remains representative.

3. **Optimizing Data Distribution**
Machine learning algorithms generally perform better when data distributions approach normality. Outliers often cause severe skewness; addressing them helps stabilize the variance.

4. **Enhancing Model Generalization**
By filtering out statistical noise and anomalies, the model focuses on learning the underlying patterns of the dataset. This leads to better performance on unseen data rather than just "memorizing" noise.

5. **Ensuring Data Integrity**
Outliers are frequently the result of data entry errors or sensor malfunctions rather than real-world phenomena. Removing these points ensures that the analysis is grounded in verified, realistic data.

### **price**
"""

# dropping price with value 0
print("Jumlah data sebelum:", len(hprices))
hprices = hprices[hprices['price'] > 0]
print("Jumlah data setelah:", len(hprices))

# log transform to handle outliers
import numpy as np
hprices['price'] = np.log1p(hprices['price'])

"""### **bedrooms**"""

# retaining bedroom counts within the range of 1-7 (to handle outliers)
hprices = hprices[(hprices['bedrooms'] > 0) & (hprices['bedrooms'] < 8)]

"""**Insights:**
- Upon inspection, houses without `bedrooms` are identified as data error outliers, as their prices are quite high and above average. Consequently, this data has been removed.

### **bathrooms**
"""

# retaining bathroom counts below 6 (to handle outliers)
hprices = hprices[(hprices['bathrooms'] < 6)]

"""### **sqft_living, sqft_above, sqft_basement**"""

from scipy.stats.mstats import winsorize

# duplicate data to avoid overwriting
hprices_winsor = hprices.copy()

# winsorizing helper function
def apply_winsorize(df, col, limits=(0.01, 0.01)):
    # winsorize only works on arrays, so we convert the Series first
    winsorized_data = winsorize(df[col], limits=limits)
    df[col] = winsorized_data
    return df

# apply winsorizing to right-skewed columns
columns_to_winsor = ['sqft_living', 'sqft_above', 'sqft_basement']
for col in columns_to_winsor:
    hprices_winsor = apply_winsorize(hprices_winsor, col)

"""## **Feature Selection**

This phase is critical for the following reasons:
1. **Preventing Execution Errors:** Eliminating non-numeric data that would otherwise cause compatibility issues during model training.
2. **Optimizing Model Efficiency:** Removing irrelevant features reduces computational overhead during training and enhances predictive accuracy by eliminating noise.
"""

# dropping columns
hprices_winsor.drop(columns=['date'], inplace=True)
hprices_winsor.drop(columns=['street'], inplace=True)
hprices_winsor.drop(columns=['statezip'], inplace=True)
hprices_winsor.drop(columns=['country'], inplace=True)
hprices_winsor.drop(columns=['sqft_lot'],inplace=True)
hprices_winsor.drop(columns=['condition'],inplace=True)
hprices_winsor.drop(columns=['yr_built'],inplace=True)
hprices_winsor.drop(columns=['yr_renovated'],inplace=True)
hprices_winsor.drop(columns=['city'],inplace=True)

"""## **Data Splitting**

This phase is critical for the following reasons:
1. **Ensuring Objective Evaluation**
The model is evaluated on unseen data, ensuring that performance metrics accurately reflect its real-world predictive capabilities.

2. **Mitigating Overfitting**
Without a separate test set, a model might perform exceptionally well on training data but fail to generalize to new, unseen instances.

3. **Facilitating Model Tuning**
A dedicated test set allows for fair validation and a standardized comparison between different algorithms.
"""

# partitioning the dataset into training and testing sets
from sklearn.model_selection import train_test_split

y = hprices_winsor['price']
X = hprices_winsor.drop(columns=['price'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Insights:**
- The implementation above partitions the dataset into:
    - 80% Training Data (X_train, y_train) → To train the model.
    - 20% Testing Data (X_test, y_test) → To evaluate model performance.

## **Standardization**

This phase is critical for the following reasons:
1. **Disparity in Feature Scales:** Features like `sqft_lot` may contain values in the tens of thousands, while others like `floors` only range from 1 to 3. Without scaling, models such as KNN can become biased toward features with larger numerical ranges.

2. **Algorithm Sensitivity to Scale and Distance:**
    - **K-Nearest Neighbors (KNN):** Relies on distance calculations between data points. Without standardization, features with higher magnitudes will disproportionately dominate the distance metric.
    - **XGBoost:** While decision-tree-based models are generally scale-invariant, standardization can still facilitate better performance and faster convergence when features exhibit vastly different distributions.

3. **Accelerating Training Convergence:** Standardization improves the stability and speed of the training process, particularly for optimization-based algorithms.
"""

# implementing standardization using StandardScaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# **Modeling**

This project implements two distinct regression algorithms to benchmark and identify the most effective predictive architecture:
1. KNN Regression
2. XGBoost Regression

### **KNN Regression**

**KNN (K-Nearest Neighbors) Regression** is a proximity-based predictive method that estimates values based on the similarity (distance) between data points. This model is non-parametric, meaning it makes no underlying assumptions about the functional relationship between features and the target variable.



**Parameter:**
- `n_neighbors=8`: Utilizes the 8 nearest neighbors for prediction. Selecting a value that is too low may result in overfitting, while an excessively high value can lead to underfitting.

**Strengths:**
- Intuitive and mathematically straightforward.
- Highly effective for capturing complex, non-linear relationships.
- Lazy learner: Does not require an explicit training phase.

**Limitations:**
- Highly sensitive to feature scales (requires robust standardization).
- Computationally expensive during inference as it calculates distances to all points in the dataset.
- Performance degrades significantly in high-dimensional feature spaces.
"""

from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor(n_neighbors=8)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

"""### **XGBoost Regression**

**XGBoost (Extreme Gradient Boosting)** is a highly efficient ensemble learning algorithm based on the gradient boosting framework. It is widely recognized for its speed and superior predictive performance in regression tasks.

**Parameters:**
- `n_estimators=100`: The number of boosting rounds (total number of decision trees).
- `learning_rate=0.1`: The step size shrinkage used to prevent overfitting.
- `max_depth=3`: The maximum depth allowed for each individual tree.

**Strengths:**
- Built-in regularization (L1 and L2) provides robust protection against overfitting.
- Exceptional at modeling complex non-linear interactions between features.
- High computational efficiency, optimized for large-scale datasets.
- Native capability to handle missing values automatically.

**Limitations:**
- High model complexity, often perceived as a "black box" regarding interpretability.
- Requires extensive hyperparameter tuning to achieve optimal results.
- Higher computational overhead compared to simpler linear models.
"""

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV

# grid configuration
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

xgb = XGBRegressor(random_state=42)

grid_search = GridSearchCV(estimator=xgb,
                           param_grid=param_grid,
                           scoring='neg_root_mean_squared_error',
                           cv=5,
                           verbose=1,
                           n_jobs=-1)

grid_search.fit(X_train, y_train)

# optimal model selection
best_xgb = grid_search.best_estimator_

# inference execution
xgb_pred = best_xgb.predict(X_test)

print("Best Parameters:")
print(grid_search.best_params_)

"""# **Evaluation**

The primary evaluation metric utilized in this project is **R-squared (R²)**, also known as the coefficient of determination.
"""

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

"""### **R-squared (R²)**

R² quantifies the proportion of variance in the target variable (`price`) that can be explained by the input features within the model. It serves as a key indicator of the model's explanatory power.

**Formula:** \[
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]

**Notation:**
- \( \bar{y} \): The mean of the actual observed values.
- **Numerator**: The Sum of Squared Residuals (the error generated by the model).
- **Denominator**: The Total Sum of Squares (the inherent variance within the data).

---

**Mechanism:** - The metric compares the total prediction error against the total variance of the target variable.
- A value approaching **1** indicates that the model is highly effective at explaining the variance in the dataset.

**Strengths:** - Provides a comprehensive overview of how well the independent variables explain the target.
- Acts as a standardized metric for holistic model evaluation.

**Interpretation:** - \( R^2 = 1 \): The model provides perfect predictions.
- \( R^2 = 0 \): The model's predictive capability is no better than simply using the mean of the data.
- \( R^2 < 0 \): The model is less effective than a horizontal line representing the mean (suggesting a poor fit).
"""

r2_knn = r2_score(y_test, knn_pred)
r2_xgb = r2_score(y_test, xgb_pred)

r2_df = pd.DataFrame({
    'Model': ['KNN', 'XGBoost'],
    'R² Score': [r2_knn, r2_xgb]
})

print("\nTabel R²:")
print(r2_df)

"""**Insights:**
- Based on the model evaluation results, **XGBoost Regression** demonstrates superior performance with an **R² of 0.91**, significantly outperforming the **KNN** model which achieved an **R² of 0.83**.
- This result indicates that the XGBoost architecture is exceptionally capable of capturing the intricate and non-linear relationships between property characteristics and market prices. This high level of accuracy provides significant value for prospective buyers and investors in estimating property values that align precisely with specific physical features and geographical locations.
"""
